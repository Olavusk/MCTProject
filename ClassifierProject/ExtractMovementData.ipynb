{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract movement data\n",
    "Here we prepare the accelerometer data from a phone (represented in a csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 22, 32)\n",
      "[  0.         120.41345528   0.           0.         121.01503267\n",
      " 111.94024624   0.           0.         111.0735851    0.\n",
      " 111.00209205 112.99096655   0.         122.34145698   0.\n",
      "   0.           0.         117.04931698   0.           0.\n",
      " 115.4117872  122.93285616   0.         109.01335435   0.\n",
      " 115.78457476   0.         106.27258509   0.          90.70400612\n",
      "   0.           0.        ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# I have given the bpm at the end of every movement file \n",
    "def extract_bpm_from_filename(filename):\n",
    "    match = re.search(r'_(\\d+)', filename)  \n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        raise ValueError(\"BPM not found in the filename.\")\n",
    "\n",
    "def prepare_velocity_profiles(filename, bars_per_segment=2):\n",
    "    bpm = extract_bpm_from_filename(filename)\n",
    "    seconds_per_beat = 60 / bpm\n",
    "    beats_per_bar = 4  \n",
    "    seconds_per_segment = seconds_per_beat * beats_per_bar * bars_per_segment\n",
    "\n",
    "    data = pd.read_csv(filename)\n",
    "    # Calculate absolute acceleration (using good ol' pythagorean)\n",
    "    data['Acceleration'] = np.sqrt(data['Acceleration x (m/s^2)']**2 + \n",
    "                                   data['Acceleration y (m/s^2)']**2 + \n",
    "                                   data['Acceleration z (m/s^2)']**2)\n",
    "    # Detect peaks in the absolute acceleration data (using a prebuilt library find_peaks)\n",
    "    height_threshold = data['Acceleration'].max() / 4\n",
    "    peaks, properties = find_peaks(data['Acceleration'], height=height_threshold, distance=50)\n",
    "    peak_values = data['Acceleration'].iloc[peaks]\n",
    "\n",
    "    # Normalize the peak values to match midi values\n",
    "    max_velocity_observed = peak_values.max()\n",
    "    normalized_velocities = (peak_values / max_velocity_observed) * 130\n",
    "\n",
    "    # Initialize the matrix array\n",
    "    total_time = data['Time (s)'].iloc[-1]\n",
    "    number_of_segments = int(total_time // seconds_per_segment)\n",
    "    matrices = np.zeros((number_of_segments, 22, 32))\n",
    "\n",
    "    # Populate the first row of each matrix with normalized peak data\n",
    "    for i, peak_time in enumerate(data['Time (s)'].iloc[peaks]):\n",
    "        segment_index = int(peak_time // seconds_per_segment)\n",
    "        if segment_index < number_of_segments:\n",
    "            time_index = int(((peak_time % seconds_per_segment) / seconds_per_segment) * 32)\n",
    "            if time_index < 32:\n",
    "                matrices[segment_index][0][time_index] = normalized_velocities.iloc[i]  # Storing normalized velocity\n",
    "\n",
    "    return matrices\n",
    "\n",
    "# Example usage (MovementData/Data6/Accelerometer_120.csv is the same data from video: https://streamable.com/e/wp8jh7)\n",
    "filepath = 'MovementData/Data6/Accelerometer_120.csv'\n",
    "matrices = prepare_velocity_profiles(filepath, bars_per_segment=2)\n",
    "\n",
    "print(matrices.shape)\n",
    "print(matrices[2][0])  # Output the chosen row of matrice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict genre\n",
    "Here we load the chosen models, and predict what genre the model believes the related rythm is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Segment 1: Predicted genre - rock, Confidence - 0.32\n",
      "Segment 2: Predicted genre - rock, Confidence - 0.33\n",
      "Segment 3: Predicted genre - rock, Confidence - 0.34\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def predict_segments(matrices, model, scaler, pca_list, genres):\n",
    "    predictions = []\n",
    "    for matrix in matrices:\n",
    "        # Flatten the matrix to match the expected input structure for scaling\n",
    "        velocity_profile = matrix.flatten().reshape(1, -1)\n",
    "        # Standardize the velocity profile\n",
    "        standardized_profile = scaler.transform(velocity_profile)\n",
    "        \n",
    "        # Prepare for PCA transformation by reshaping into num_instruments x num_timesteps\n",
    "        standardized_profile_reshaped = standardized_profile.reshape(-1, 22, 32)\n",
    "\n",
    "        # Apply PCA transformation on each timestep across all instruments\n",
    "        pca_transformed = np.hstack([pca_list[i].transform(standardized_profile_reshaped[:, :, i]) for i in range(len(pca_list))])\n",
    "        \n",
    "        # Reshape for model input\n",
    "        pca_transformed = pca_transformed.reshape(1, -1, 1)\n",
    "        \n",
    "        # Make predictions\n",
    "        prediction = model.predict(pca_transformed)\n",
    "        predicted_class = np.argmax(prediction, axis=1)\n",
    "        predicted_genre = genres[predicted_class[0]]\n",
    "        confidence = prediction[0][predicted_class[0]]\n",
    "        \n",
    "        # Store results\n",
    "        predictions.append((predicted_genre, confidence))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Load models and preprocessing objects (these are premade, can make more in drumclassifier_PCA_CNN (just make sure to update genres to match))\n",
    "model = load_model('models/pca_cnn/pca_cnn_model_10G.keras')\n",
    "scaler = joblib.load('models/pca_cnn/scaler_10G.pkl')\n",
    "pca_list = joblib.load('models/pca_cnn/pca_list_10G.pkl')  \n",
    "\n",
    "matrices = prepare_velocity_profiles(filepath, bars_per_segment=2)\n",
    "genres = ['latin',  'jazz',  'punk', 'afrobeat', 'hiphop', 'soul', 'funk', 'rock', 'country', 'reggae']\n",
    "\n",
    "# Predict for all segments\n",
    "segment_predictions = predict_segments(matrices, model, scaler, pca_list, genres)\n",
    "\n",
    "# Output predictions ()\n",
    "for i, (genre, confidence) in enumerate(segment_predictions):\n",
    "    print(f\"Segment {i+1}: Predicted genre - {genre}, Confidence - {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA_LDA_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Segment 1: Predicted genre - rock, Confidence - 0.39\n",
      "Segment 2: Predicted genre - rock, Confidence - 0.40\n",
      "Segment 3: Predicted genre - rock, Confidence - 0.40\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def predict_segments(matrices, model, scaler, pca, lda, genres):\n",
    "    predictions = []\n",
    "    for matrix in matrices:\n",
    "        # Reducing matrix by averaging over the 32 timesteps to get 22 features\n",
    "        reduced_matrix = np.mean(matrix, axis=1).reshape(1, -1)\n",
    "\n",
    "        # Standardize the velocity profile\n",
    "        standardized_profile = scaler.transform(reduced_matrix)\n",
    "\n",
    "        # Apply PCA transformation\n",
    "        pca_transformed = pca.transform(standardized_profile)\n",
    "\n",
    "        # Apply LDA transformation\n",
    "        lda_transformed = lda.transform(pca_transformed)\n",
    "\n",
    "        # Artificially extend the LDA output to match model input expectations\n",
    "        lda_transformed_extended = np.tile(lda_transformed, (32, 1)).reshape(1, 32, 1)\n",
    "\n",
    "        # Make predictions\n",
    "        prediction = model.predict(lda_transformed_extended)\n",
    "        predicted_class = np.argmax(prediction, axis=1)\n",
    "        predicted_genre = genres[predicted_class[0]]\n",
    "        confidence = prediction[0][predicted_class[0]]\n",
    "\n",
    "        # Store results\n",
    "        predictions.append((predicted_genre, confidence))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "model = load_model('models/pca_lda_cnn/pca_lda_cnn_model_10G.keras') \n",
    "scaler = joblib.load('models/pca_lda_cnn/scaler_10G.pkl')  \n",
    "pca = joblib.load('models/pca_lda_cnn/pca_model_10G.pkl')  \n",
    "lda = joblib.load('models/pca_lda_cnn/lda_model_10G.pkl') \n",
    "\n",
    "# Using the updated function\n",
    "segment_predictions = predict_segments(matrices, model, scaler, pca, lda, genres)\n",
    "\n",
    "# Output predictions\n",
    "for i, (genre, confidence) in enumerate(segment_predictions):\n",
    "    print(f\"Segment {i+1}: Predicted genre - {genre}, Confidence - {confidence:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
